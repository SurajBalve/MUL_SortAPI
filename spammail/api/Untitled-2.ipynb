{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"Hey! Do you want to get rich quick? Call 123-456-7890 now!!\", \n",
    "        \"This is a legitimate message from your friend. Let's meet up!\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\".join(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hey! do you want to get rich quick? call 123-456-7890 now!!this is a legitimate message from your friend. let's meet up!\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey do you want to get rich quick call 1234567890 nowthis is a legitimate message from your friend lets meet up'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "text= text.translate(str.maketrans('','',string.punctuation))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey do you want to get rich quick call 1234567890 nowthis is a legitimate message from your friend lets meet up'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "# text= re.sub(r'\\d+','',text) # re.sub(pattern, repl, string) \\d = represent any digit\n",
    "# r=   before the string indicates a raw string literal in Python, meaning special characters like backslashes (\\) are treated literally, not as escape characters. This makes it easier to write regular expressions without having to escape backslashes\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization splits the text into individual words or tokens. Each word or token is now treated as an individual unit, which is essential for machine learning models.from  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey do you want to get rich quick call 1234567890 nowthis is a legitimate message from your friend lets meet up'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey do you want to get rich quick call  nowthis is a legitimate message from your friend lets meet up'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = re.sub(r'\\d+', '', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey do you want to get rich quick call nowthis is a legitimate message from your friend lets meet up'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = re.sub(r'\\s+', ' ', text).strip()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 19 stored elements and shape (20, 19)>\n",
      "  Coords\tValues\n",
      "  (0, 5)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 17)\t1.0\n",
      "  (3, 16)\t1.0\n",
      "  (4, 14)\t1.0\n",
      "  (5, 4)\t1.0\n",
      "  (6, 13)\t1.0\n",
      "  (7, 12)\t1.0\n",
      "  (8, 0)\t1.0\n",
      "  (9, 11)\t1.0\n",
      "  (10, 6)\t1.0\n",
      "  (12, 7)\t1.0\n",
      "  (13, 10)\t1.0\n",
      "  (14, 3)\t1.0\n",
      "  (15, 18)\t1.0\n",
      "  (16, 2)\t1.0\n",
      "  (17, 8)\t1.0\n",
      "  (18, 9)\t1.0\n",
      "  (19, 15)\t1.0\n"
     ]
    }
   ],
   "source": [
    "#Tokenization splits the text into individual words or tokens. Each word or token is now treated as an individual unit, which is essential for machine learning models.from\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "text= text.split()\n",
    "\n",
    "vectorizer= TfidfVectorizer()\n",
    "x_train = vectorizer.fit(text)\n",
    "x_test=vectorizer.transform(text)\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
